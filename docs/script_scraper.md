Com certeza! Aqui estÃ¡ o arquivo Markdown atualizado para refletir o novo cÃ³digo e incluir as observaÃ§Ãµes sobre a estratÃ©gia de retry e as boas prÃ¡ticas para nÃ£o sobrecarregar o servidor.

---

# Fluxo do Script de Scraping - AsyncBookScraper

O fluxo estÃ¡ organizado de forma hierÃ¡rquica, mostrando cada etapa do processo de extraÃ§Ã£o de dados. Os pontos principais sÃ£o:

**ğŸ”§ Setup â†’ ğŸ“Š Descoberta â†’ ğŸŒ Busca Paralela â†’ ğŸ” Parsing Detalhado â†’ ğŸ’¾ PersistÃªncia**

A grande vantagem do script Ã© o **duplo paralelismo**: primeiro, busca todas as pÃ¡ginas de listagem simultaneamente; depois, faz o parsing detalhado de cada livro, que inclui uma nova requisiÃ§Ã£o Ã  pÃ¡gina de detalhes de cada livro, tambÃ©m em paralelo. Isso resulta em uma performance excelente.

## ğŸ“‹ Resumo

Script assÃ­ncrono em Python que extrai dados de livros do site `books.toscrape.com`, gerencia a concorrÃªncia de requisiÃ§Ãµes, implementa uma estratÃ©gia de retry e salva os dados consolidados em um arquivo CSV.

## ğŸŒ³ Estrutura do Fluxo

### ğŸ“¦ `AsyncBookScraper.run()`

*   **ğŸ”§ ConfiguraÃ§Ã£o Inicial**
    *   Criar semÃ¡foro (`asyncio.Semaphore`) para controlar a concorrÃªncia.
    *   Configurar sessÃ£o HTTP (`aiohttp`) com a biblioteca `aiohttp-retry`, definindo uma estratÃ©gia de **retry exponencial**.
    *   Definir `HEADERS` para simular um navegador real.

*   **ğŸ“Š Descoberta de Estrutura**
    *   `_get_total_pages()`
        *   Acessar a pÃ¡gina inicial.
        *   Encontrar o elemento de paginaÃ§Ã£o (`li.current`).
        *   Extrair o nÃºmero total de pÃ¡ginas via regex.
        *   LanÃ§ar exceÃ§Ã£o customizada (`PaginatorNotFoundException`) se o paginador nÃ£o for encontrado.

*   **ğŸ”— GeraÃ§Ã£o de URLs**
    *   `_generate_page_urls()`
        *   Criar uma lista de URLs para todas as pÃ¡ginas de catÃ¡logo.
        *   Formato: `/catalogue/page-{N}.html`

*   **ğŸŒ Busca AssÃ­ncrona de PÃ¡ginas de Listagem**
    *   `_fetch_all_pages()`
        *   Criar uma `task` para cada URL de pÃ¡gina de listagem.
        *   Executar todas as requisiÃ§Ãµes simultaneamente usando `asyncio.gather`.
        *   Filtrar respostas que falharam, garantindo a robustez.

*   **ğŸ“š ExtraÃ§Ã£o de Elementos de Livros**
    *   `_extract_books_from_pages()`
        *   Para cada pÃ¡gina HTML, encontrar todos os elementos `<article class="product_pod">`.
        *   Consolidar uma lista Ãºnica com todos os elementos de livros encontrados.

*   **ğŸ” Parsing Detalhado (AssÃ­ncrono)**
    *   `_parse_all_books()` -> Para cada livro â†’ `_parse_book()`
        *   **1. Extrair dados bÃ¡sicos da lista:**
            *   TÃ­tulo, PreÃ§o, Disponibilidade e Rating.
        *   **2. Acessar pÃ¡gina individual do livro (nova requisiÃ§Ã£o assÃ­ncrona):**
            *   Chama `_get_image_path_and_category()`, que faz uma nova requisiÃ§Ã£o para a URL de detalhes do livro.
        *   **3. Extrair dados detalhados:**
            *   Categoria e URL da Imagem.

*   **ğŸ’¾ Salvamento**
    *   `_save_to_csv()`
        *   Criar arquivo `books.csv` no local definido.
        *   Escrever o cabeÃ§alho e os dados de todos os livros.
        *   Confirmar a conclusÃ£o da escrita no console.

## âš¡ CaracterÃ­sticas TÃ©cnicas

### ConcorrÃªncia
*   **SemÃ¡foro:** Limita o nÃºmero mÃ¡ximo de requisiÃ§Ãµes simultÃ¢neas (padrÃ£o: 15) para nÃ£o sobrecarregar o servidor.
*   **Async/Await:** Utiliza operaÃ§Ãµes de I/O nÃ£o-bloqueantes para mÃ¡xima eficiÃªncia.
*   **Gather:** Executa mÃºltiplas `tasks` (busca de pÃ¡ginas e parsing de livros) em paralelo.

### Robustez e Respeito ao Servidor
*   **EstratÃ©gia de Retry:**
    *   Utiliza a biblioteca `aiohttp-retry` com uma polÃ­tica de `ExponentialRetry`.
    *   **Tentativas:** Tenta novamente atÃ© 5 vezes em caso de erros de servidor (status `500`, `502`, `503`, `504`).
    *   **Backoff:** O tempo de espera entre as tentativas aumenta exponencialmente, uma prÃ¡tica recomendada para dar tempo ao servidor de se recuperar.
*   **Timeout:** Define um timeout de 5 segundos por requisiÃ§Ã£o para evitar que o script fique preso.
*   **Tratamento de Erro:** ExceÃ§Ãµes customizadas (`ScraperException`) para falhas crÃ­ticas e tratamento de erros de requisiÃ§Ã£o individuais, permitindo que o script continue a processar os dados vÃ¡lidos.
*   **User-Agent:** Define um `User-Agent` comum para identificar o scraper como um cliente web padrÃ£o, uma prÃ¡tica de boa vizinhanÃ§a.

### Performance
*   **2 NÃ­veis de Paralelismo:**
    1.  **Busca de pÃ¡ginas de listagem:** Todas as 50 pÃ¡ginas de catÃ¡logo sÃ£o baixadas em paralelo.
    2.  **Parsing de livros:** O parsing de cada um dos 1000 livros, que inclui uma requisiÃ§Ã£o Ã  sua pÃ¡gina de detalhes, tambÃ©m ocorre em paralelo.
*   **Filtros:** Remove pÃ¡ginas ou dados invÃ¡lidos para garantir a qualidade do resultado final.

## ğŸ“ˆ Fluxo de Dados

`URL Base â†’ PÃ¡ginas de Lista â†’ Elementos HTML â†’ RequisiÃ§Ãµes de Detalhe â†’ Dados Estruturados â†’ CSV`

```
books.toscrape.com
        â†“
   50 pÃ¡ginas de listagem HTML (em paralelo)
        â†“
   ~1000 elementos <article>
        â†“
   ~1000 requisiÃ§Ãµes para pÃ¡ginas de detalhe (em paralelo)
        â†“
   Dados extraÃ­dos por livro:
   â€¢ ID, TÃ­tulo, PreÃ§o, Disponibilidade
   â€¢ Rating, Categoria, URL da Imagem
        â†“
   books.csv
```

## ğŸ¯ Resultado Final

*   **Arquivo:** `books.csv`
*   **LocalizaÃ§Ã£o:** DiretÃ³rio definido nas configuraÃ§Ãµes (ex: `data/`).
*   **ConteÃºdo:** Dados de todos os livros encontrados no site.
*   **Formato:** CSV com cabeÃ§alhos (`id`, `title`, `price`, etc.).